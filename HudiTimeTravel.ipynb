{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edd7973-3c88-4d92-8b2f-c69e8282c1ee",
   "metadata": {},
   "source": [
    "# Apache Hudi Query Tests\n",
    "\n",
    "Author: Gary Stafford\n",
    "Date: 2022-12-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c118d6a-ebe1-423a-a6a7-53b6c350d2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Amazon EMR cluster configuration used for this notebook\n",
    "\n",
    "# aws emr create-cluster \\\n",
    "#     --os-release-label 2.0.20221103.3 --termination-protected \\\n",
    "#     --applications Name=Hadoop Name=Spark Name=JupyterEnterpriseGateway Name=Livy Name=JupyterHub \\\n",
    "#     --ec2-attributes '{\"KeyName\":\"emr-demo-111222333444-us-east-1\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-067e7e7ee0fd99b2b\",\"EmrManagedSlaveSecurityGroup\":\"sg-0cc2191e5af867588\",\"EmrManagedMasterSecurityGroup\":\"sg-0be73448b3814bbc7\"}' \\\n",
    "#     --release-label emr-6.8.0 --log-uri 's3n://aws-logs-111222333444-us-east-1/elasticmapreduce/' \\\n",
    "#     --instance-groups '[{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":2}]},\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Master - 1\"},{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":2}]},\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Core - 2\"}]' \\\n",
    "#     --configurations '[{\"Classification\":\"spark-hive-site\",\"Properties\":{\"hive.metastore.client.factory.class\":\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\"}}]' \\\n",
    "#     --auto-scaling-role EMR_AutoScaling_DefaultRole --ebs-root-volume-size 100 --service-role EMR_DefaultRole --enable-debugging \\\n",
    "#     --name 'DemoCluster_6_8' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d00054-d9be-4ae5-8557-ab23ff52a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplied config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bca72-ce43-4eae-8334-0806a95eb4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%configure -f\n",
    "\n",
    "# { \"conf\": {\n",
    "#         \"spark.pyspark.virtualenv.enabled\": \"false\",\n",
    "#         \"spark.jars\": \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar\",\n",
    "#         \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "#         \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "#         \"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "#         \"spark.hadoop.hive.metastore.client.factory.class\": \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\",\n",
    "#         \"spark.hadoop.hive.metastore.schema.verification\": \"false\",\n",
    "#         \"spark.hadoop.fs.s3.canned.acl\": \"AuthenticatedRead\",\n",
    "#         \"spark.sql.catalogImplementation\": \"hive\"\n",
    "# }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c223a09-197b-4af9-ba64-dda37e6ccc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default aws hudi emr config\n",
    "# works fine\n",
    "# https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f3c9d-d8ad-4778-b3da-44585f2eadc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%configure -f\n",
    "\n",
    "# {\n",
    "#     \"conf\": {\n",
    "#         \"spark.jars\":\n",
    "#             \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "#         \"spark.serializer\":\n",
    "#             \"org.apache.spark.serializer.KryoSerializer\",\n",
    "#         \"spark.sql.hive.convertMetastoreParquet\":\n",
    "#             \"false\"\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221ea80-eb34-4605-91bb-6391aabedc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined configs - default aws hudi emr config and emr glue data catalog config and hudi spark config\n",
    "# https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html\n",
    "# https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/metastore-config.html#glue-metastore\n",
    "# https://hudi.apache.org/docs/quick-start-guide/#setup\n",
    "\n",
    "# Note that \"spark.hadoop.hive.metastore.client.factory.class\" is extraneous since it was set in the EMR cluster config when provisioned (see first cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181640c1-a8ec-463b-bf50-3429ac705af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:48:34.682800Z",
     "iopub.status.busy": "2022-12-09T04:48:34.682566Z",
     "iopub.status.idle": "2022-12-09T04:48:34.710861Z",
     "shell.execute_reply": "2022-12-09T04:48:34.710206Z",
     "shell.execute_reply.started": "2022-12-09T04:48:34.682774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.hadoop.hive.metastore.client.factory.class': 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.hudi.catalog.HoodieCatalog', 'spark.sql.extensions': 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension'}, 'proxyUser': 'assumed-role_AWSReservedSSO_AdministratorAccess_a3e1b00336d11e27_GaryStaffordAdminPersonal', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>10</td><td>application_1670528992887_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0011/\" class=\"emr-proxy-link\" emr-resource=\"j-26M1ZRMTF8QYV\n",
       "\" application-id=\"application_1670528992887_0011\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-51.ec2.internal:8042/node/containerlogs/container_1670528992887_0011_01_000001/livy\" >Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\":\n",
    "            \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "        \"spark.serializer\":\n",
    "            \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.hive.convertMetastoreParquet\":\n",
    "            \"false\",\n",
    "        \"spark.hadoop.hive.metastore.client.factory.class\":\n",
    "            \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\",\n",
    "        \"spark.sql.catalog.spark_catalog\":\n",
    "            \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "        \"spark.sql.extensions\":\n",
    "            \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfaae400-5d9a-4b0d-bdff-3789a46d1990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:48:36.869775Z",
     "iopub.status.busy": "2022-12-09T04:48:36.869556Z",
     "iopub.status.idle": "2022-12-09T04:49:06.540955Z",
     "shell.execute_reply": "2022-12-09T04:49:06.540326Z",
     "shell.execute_reply.started": "2022-12-09T04:48:36.869751Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ff30aab7947479c881184e983305e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>15</td><td>application_1670528992887_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0016/\" class=\"emr-proxy-link\" emr-resource=\"j-26M1ZRMTF8QYV\n",
       "\" application-id=\"application_1670528992887_0016\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-51.ec2.internal:8042/node/containerlogs/container_1670528992887_0016_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8b2568-713a-4fdd-b5a3-4fa1eb8abde8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:50:05.441688Z",
     "iopub.status.busy": "2022-12-09T04:50:05.441456Z",
     "iopub.status.idle": "2022-12-09T04:50:05.706767Z",
     "shell.execute_reply": "2022-12-09T04:50:05.706051Z",
     "shell.execute_reply.started": "2022-12-09T04:50:05.441662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1384a0c367294a9da4291a1f7d651234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.sql.hive.convertMetastoreParquet', 'false'), ('spark.eventLog.enabled', 'true'), ('spark.driver.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'), ('spark.blacklist.decommissioning.timeout', '1h'), ('spark.yarn.tags', 'livy-session-15-FrerhDuC'), ('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)'), ('spark.sql.warehouse.dir', 'hdfs://ip-10-192-10-139.ec2.internal:8020/user/spark/warehouse'), ('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.eventLog.dir', 'hdfs:///var/log/spark/apps'), ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps'), ('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), ('spark.executor.memory', '4743M'), ('spark.yarn.historyServer.address', 'ip-10-192-10-139.ec2.internal:18080'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0016'), ('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.app.name', 'livy-session-15'), ('spark.repl.class.outputDir', '/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/tmp/spark2297586597597710733'), ('spark.emr.default.executor.memory', '4743M'), ('spark.hadoop.yarn.timeline-service.enabled', 'false'), ('spark.emr.default.executor.cores', '2'), ('spark.executor.id', 'driver'), ('spark.hadoop.hive.metastore.client.factory.class', 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'), ('spark.driver.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'), ('spark.app.startTime', '1670561333386'), ('spark.driver.memory', '2048M'), ('spark.executorEnv.PYTHONPATH', '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.5-src.zip'), ('spark.yarn.app.container.log.dir', '/var/log/hadoop-yarn/containers/application_1670528992887_0016/container_1670528992887_0016_01_000001'), ('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true'), ('spark.decommissioning.timeout.threshold', '20'), ('spark.sql.catalogImplementation', 'hive'), ('spark.yarn.dist.pyFiles', ''), ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'), ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2'), ('spark.driver.port', '41513'), ('spark.submit.deployMode', 'cluster'), ('spark.yarn.submit.waitAppCompletion', 'false'), ('spark.yarn.dist.files', 'file:/etc/hudi/conf.dist/hudi-defaults.conf'), ('spark.yarn.dist.archives', 'file:/usr/lib/spark/R/lib/sparkr.zip#sparkr'), ('spark.yarn.secondary.jars', 'arpack_combined_all-0.1.jar,core-1.1.2.jar,jniloader-1.1.jar,livy-api-0.7.1-incubating.jar,livy-rsc-0.7.1-incubating.jar,livy-thriftserver-session-0.7.1-incubating.jar,native_ref-java-1.1.jar,native_system-java-1.1.jar,netlib-native_ref-linux-armhf-1.1-natives.jar,netlib-native_ref-linux-i686-1.1-natives.jar,netlib-native_ref-linux-x86_64-1.1-natives.jar,netlib-native_ref-osx-x86_64-1.1-natives.jar,netlib-native_ref-win-i686-1.1-natives.jar,netlib-native_ref-win-x86_64-1.1-natives.jar,netlib-native_system-linux-armhf-1.1-natives.jar,netlib-native_system-linux-i686-1.1-natives.jar,netlib-native_system-linux-x86_64-1.1-natives.jar,netlib-native_system-osx-x86_64-1.1-natives.jar,netlib-native_system-win-i686-1.1-natives.jar,netlib-native_system-win-x86_64-1.1-natives.jar,netty-all-4.1.17.Final.jar,hudi-spark-bundle.jar,spark-avro.jar,commons-codec-1.9.jar,livy-core_2.12-0.7.1-incubating.jar,livy-repl_2.12-0.7.1-incubating.jar'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'ip-10-192-10-139.ec2.internal'), ('spark.yarn.maxAppAttempts', '1'), ('spark.repl.class.uri', 'spark://ip-10-192-10-51.ec2.internal:41513/classes'), ('spark.yarn.app.id', 'application_1670528992887_0016'), ('spark.sql.hive.metastore.sharedPrefixes', 'com.amazonaws.services.dynamodbv2'), ('spark.driver.host', 'ip-10-192-10-51.ec2.internal'), ('spark.app.attempt.id', '1'), ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'), ('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.app.id', 'application_1670528992887_0016'), ('spark.livy.spark_major_version', '3'), ('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'), ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true'), ('spark.history.ui.port', '18080'), ('spark.shuffle.service.enabled', 'true'), ('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog'), ('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.resourceManager.cleanupExpiredHost', 'true'), ('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.executor.cores', '2'), ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'), ('spark.master', 'yarn'), ('spark.ui.port', '0'), ('spark.app.submitTime', '1670561319686'), ('spark.executor.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.submit.pyFiles', ''), ('spark.dynamicAllocation.enabled', 'true'), ('spark.yarn.isPython', 'true'), ('spark.yarn.dist.jars', 'file:///usr/lib/livy/rsc-jars/arpack_combined_all-0.1.jar,file:///usr/lib/livy/rsc-jars/core-1.1.2.jar,file:///usr/lib/livy/rsc-jars/jniloader-1.1.jar,file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/native_ref-java-1.1.jar,file:///usr/lib/livy/rsc-jars/native_system-java-1.1.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-linux-armhf-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-linux-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-linux-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-osx-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-win-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-win-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-linux-armhf-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-linux-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-linux-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-osx-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-win-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-win-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,hdfs://ip-10-192-10-139.ec2.internal:8020/apps/hudi/lib/hudi-spark-bundle.jar,hdfs://ip-10-192-10-139.ec2.internal:8020/apps/hudi/lib/spark-avro.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar'), ('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension'), ('spark.blacklist.decommissioning.enabled', 'true')]"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d335cbb-8b1b-4639-bbb5-62f0bc371015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:50:49.293366Z",
     "iopub.status.busy": "2022-12-09T04:50:49.293131Z",
     "iopub.status.idle": "2022-12-09T04:50:49.345933Z",
     "shell.execute_reply": "2022-12-09T04:50:49.345314Z",
     "shell.execute_reply.started": "2022-12-09T04:50:49.293339Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4e41608b754dbfaa05f5142130053c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# amazon s3 path to hudi table key (sub-directory)\n",
    "\n",
    "base_path = 's3://open-data-lake-demo-us-east-1/hudi_demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371f070-3c87-4e6e-8d96-98a902e1703e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html#emr-hudi-dataframe\n",
    "\n",
    "# Create a DataFrame\n",
    "input_df = spark.createDataFrame([\n",
    "    (\"100\", \"2015-01-01\", \"2015-01-02T13:51:39.340396Z\"),\n",
    "    (\"101\", \"2015-01-01\", \"2015-01-02T12:14:58.597216Z\"),\n",
    "    (\"102\", \"2015-01-01\", \"2015-01-02T13:51:40.417052Z\"),\n",
    "    (\"103\", \"2015-01-01\", \"2015-01-02T13:51:40.519832Z\"),\n",
    "    (\"104\", \"2015-01-02\", \"2015-01-02T12:15:00.512679Z\"),\n",
    "    (\"105\", \"2015-01-02\", \"2015-01-02T13:51:42.248818Z\"),\n",
    "], [\"id\", \"creation_date\", \"last_update_time\"])\n",
    "\n",
    "# Specify common DataSourceWriteOptions in the single hudi options variable\n",
    "# https://hudi.apache.org/docs/configurations\n",
    "hudi_write_options = {\n",
    "    'hoodie.datasource.write.operation':\n",
    "        'upsert',\n",
    "    'hoodie.table.name':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.write.table.type':\n",
    "        'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field':\n",
    "        'id',\n",
    "    'hoodie.datasource.write.partitionpath.field':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.write.precombine.field':\n",
    "        'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable':\n",
    "        'true',\n",
    "    'hoodie.datasource.hive_sync.database':\n",
    "        'default',\n",
    "    'hoodie.datasource.hive_sync.table':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.hive_sync.partition_fields':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class':\n",
    "        'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.hive_sync.mode':\n",
    "        'hms'\n",
    "}\n",
    "\n",
    "# Write a DataFrame as a Hudi dataset\n",
    "input_df.write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**hudi_write_options) \\\n",
    "    .mode('append') \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239590eb-cfd5-402c-84fd-a5e9bdaafeaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "input_df = spark.createDataFrame([\n",
    "    (\"110\", \"2016-01-02\", \"2016-01-02T13:51:39.340396Z\"),\n",
    "    (\"111\", \"2016-01-02\", \"2016-01-02T12:14:58.597216Z\"),\n",
    "    (\"112\", \"2016-01-02\", \"2016-01-02T13:51:40.417052Z\"),\n",
    "    (\"113\", \"2016-01-02\", \"2016-01-02T13:51:40.519832Z\"),\n",
    "    (\"114\", \"2016-01-02\", \"2016-01-02T12:15:00.512679Z\"),\n",
    "    (\"115\", \"2016-01-02\", \"2016-01-02T13:51:42.248818Z\"),\n",
    "], [\"id\", \"creation_date\", \"last_update_time\"])\n",
    "\n",
    "# Specify common DataSourceWriteOptions in the single hudi options variable\n",
    "# https://hudi.apache.org/docs/configurations\n",
    "hudi_write_options = {\n",
    "    'hoodie.datasource.write.operation':\n",
    "        'upsert',\n",
    "    'hoodie.table.name':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.write.table.type':\n",
    "        'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field':\n",
    "        'id',\n",
    "    'hoodie.datasource.write.partitionpath.field':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.write.precombine.field':\n",
    "        'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable':\n",
    "        'true',\n",
    "    'hoodie.datasource.hive_sync.database':\n",
    "        'default',\n",
    "    'hoodie.datasource.hive_sync.table':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.hive_sync.partition_fields':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class':\n",
    "        'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.hive_sync.mode':\n",
    "        'hms'\n",
    "}\n",
    "\n",
    "# Write a DataFrame as a Hudi dataset\n",
    "input_df.write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**hudi_write_options) \\\n",
    "    .mode('append') \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2520f24-55d6-4994-9e9c-5b9de48f6e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:06.023104Z",
     "iopub.status.busy": "2022-12-09T04:51:06.022834Z",
     "iopub.status.idle": "2022-12-09T04:51:19.344484Z",
     "shell.execute_reply": "2022-12-09T04:51:19.343655Z",
     "shell.execute_reply.started": "2022-12-09T04:51:06.023076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73a072897944341a26fb9ba90baf496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|creation_date|    last_update_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|   2016-01-02|2016-01-02T12:15:...|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|   2016-01-02|2016-01-02T12:14:...|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|   2015-01-01|2015-01-02T12:14:...|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|   2015-01-02|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|   2015-01-02|2015-01-02T12:15:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from s3\n",
    "\n",
    "snapshot_query_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load(base_path + '/*/*')\n",
    "\n",
    "snapshot_query_df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717a6e1b-8dcd-4328-809e-7430d1061a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:32.867784Z",
     "iopub.status.busy": "2022-12-09T04:51:32.867560Z",
     "iopub.status.idle": "2022-12-09T04:51:38.525853Z",
     "shell.execute_reply": "2022-12-09T04:51:38.525148Z",
     "shell.execute_reply.started": "2022-12-09T04:51:32.867759Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccaaba3349243c297fee346fa11aa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|2015-01-02T12:14:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|2015-01-02T13:51:...|   2015-01-02|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|2015-01-02T12:15:...|   2015-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from glue data catalog\n",
    "\n",
    "glue_query_df = spark.sql(\"select * from `default`.`my_hudi_table`;\")\n",
    "\n",
    "glue_query_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a342c5-1a7d-4f93-86bf-b252f95191e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:38.527185Z",
     "iopub.status.busy": "2022-12-09T04:51:38.527004Z",
     "iopub.status.idle": "2022-12-09T04:51:39.802743Z",
     "shell.execute_reply": "2022-12-09T04:51:39.801728Z",
     "shell.execute_reply.started": "2022-12-09T04:51:38.527155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412b657cb43f45ff88ddde0d73666591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|2015-01-02T12:14:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|2015-01-02T13:51:...|   2015-01-02|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|2015-01-02T12:15:...|   2015-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from glue data catalog\n",
    "\n",
    "as_of_time = '20221209033946565'\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('as.of.instant', as_of_time) \\\n",
    "    .table('default.my_hudi_table')\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78027b66-7e58-4729-ab22-d8921b659780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:39.804962Z",
     "iopub.status.busy": "2022-12-09T04:51:39.804699Z",
     "iopub.status.idle": "2022-12-09T04:51:40.594179Z",
     "shell.execute_reply": "2022-12-09T04:51:40.593495Z",
     "shell.execute_reply.started": "2022-12-09T04:51:39.804925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c997691ca877415387078bf150290931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o90.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:71)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1034, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o90.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:71)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from glue data catalog as of a date/time using SparkSQL\n",
    "# throws error: java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
    "\n",
    "as_of_time = '20221209005913239'\n",
    "\n",
    "incremental_df = spark.sql(\n",
    "    'select * from default.my_hudi_table timestamp as of ' + as_of_time + ';')\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebaceb5-a6b9-4461-b554-f0d0566f5066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:40.596713Z",
     "iopub.status.busy": "2022-12-09T04:51:40.596524Z",
     "iopub.status.idle": "2022-12-09T04:51:41.349599Z",
     "shell.execute_reply": "2022-12-09T04:51:41.348817Z",
     "shell.execute_reply.started": "2022-12-09T04:51:40.596682Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o90.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:71)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1034, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1670528992887_0016/container_1670528992887_0016_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o90.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:71)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "select * from default.my_hudi_table timestamp as of 20221209005913239;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68985f5a-5984-42cd-a8b3-9b49b6a979ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:41.351149Z",
     "iopub.status.busy": "2022-12-09T04:51:41.350868Z",
     "iopub.status.idle": "2022-12-09T04:51:46.639224Z",
     "shell.execute_reply": "2022-12-09T04:51:46.638405Z",
     "shell.execute_reply.started": "2022-12-09T04:51:41.351113Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad55247a1ece49f1a118504bdadb2ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|creation_date|    last_update_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|   2016-01-02|2016-01-02T12:15:...|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|   2016-01-02|2016-01-02T12:14:...|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|   2015-01-01|2015-01-02T12:14:...|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|   2015-01-02|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|   2015-01-02|2015-01-02T12:15:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# incremental query\n",
    "# load data into a dataframe from glue data catalog starting as a date/time\n",
    "# https://hudi.apache.org/docs/quick-start-guide/#incremental-query\n",
    "\n",
    "instant_time = '20221208000000000'\n",
    "\n",
    "incremental_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': instant_time,\n",
    "}\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**incremental_read_options) \\\n",
    "    .load(base_path)\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89dd7d80-c405-457d-9846-aab25b09024b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:46.640679Z",
     "iopub.status.busy": "2022-12-09T04:51:46.640428Z",
     "iopub.status.idle": "2022-12-09T04:51:48.933019Z",
     "shell.execute_reply": "2022-12-09T04:51:48.932228Z",
     "shell.execute_reply.started": "2022-12-09T04:51:46.640642Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470b424c71d747d4b8284b120b766def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# add contents of dataframe to a temporary view and then query using SparkSQL\n",
    "\n",
    "incremental_df.createOrReplaceTempView('hudi_incremental')\n",
    "\n",
    "spark.sql(\n",
    "    'select * from `default`.`my_hudi_table` where id > 110 sort by id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786c6fc1-d033-4908-b425-fc3d45768292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:48.934527Z",
     "iopub.status.busy": "2022-12-09T04:51:48.934276Z",
     "iopub.status.idle": "2022-12-09T04:51:54.286975Z",
     "shell.execute_reply": "2022-12-09T04:51:54.286161Z",
     "shell.execute_reply.started": "2022-12-09T04:51:48.934492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2f5990ddb546f5b968adfbde25b326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|_hoodie_commit_time|count|\n",
      "+-------------------+-----+\n",
      "|  20221209033946565|    6|\n",
      "|  20221208185224525|    6|\n",
      "+-------------------+-----+\n",
      "\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221208191527968|20221208191527968...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|2015-01-02T12:14:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|2015-01-02T13:51:...|   2015-01-02|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|2015-01-02T12:15:...|   2015-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# time travel query\n",
    "# https://hudi.apache.org/docs/0.11.1/quick-start-guide/#time-travel-query\n",
    "\n",
    "spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load(base_path) \\\n",
    "    .groupBy('_hoodie_commit_time').count().show()\n",
    "\n",
    "instant0 = '2022-12-08 00:00:00.000'  # before any records were created - should see no records\n",
    "\n",
    "instant1 = '2022-12-08 18:08:27.522'  # ids 100 - 105 created  - should see (6) new records\n",
    "\n",
    "instant2 = '2022-12-08 18:13:30.830'  # ids 110 - 115 created - should see (12) new records\n",
    "\n",
    "instant3 = '2022-12-08 18:52:24.525'  # ids 100 - 105 updated - should see (6) new records and (6) modified records\n",
    "\n",
    "instant4 = '2022-12-08 19:15:27.968'  # ids 110 - 115 updated - should see (12) modified records\n",
    "\n",
    "instant5 = '2022-12-09 00:59:13.239'  # ids 110 - 115 updated second time - should see (12) modified records\n",
    "\n",
    "instant6 = '2022-12-09 03:39:46.565'  # ids 110 - 115 updated third time - should see (12) modified records\n",
    "\n",
    "instant7 = '2022 12 09 00:00:00.000'  # after latest changes - should see (12) modified/latest records\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('as.of.instant', instant4) \\\n",
    "    .load(base_path)\n",
    "\n",
    "incremental_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8e75f6-740d-4561-8281-b7376f3ce4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:54.288990Z",
     "iopub.status.busy": "2022-12-09T04:51:54.288808Z",
     "iopub.status.idle": "2022-12-09T04:51:56.599323Z",
     "shell.execute_reply": "2022-12-09T04:51:56.597502Z",
     "shell.execute_reply.started": "2022-12-09T04:51:54.288966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f9ce551200498aac7308fd0be1fc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|creation_date|    last_update_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|  20221208191527968|20221208191527968...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|   2016-01-02|2016-01-02T12:15:...|\n",
      "|  20221208191527968|20221208191527968...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208191527968|20221208191527968...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208191527968|20221208191527968...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|   2016-01-02|2016-01-02T12:14:...|\n",
      "|  20221208191527968|20221208191527968...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208191527968|20221208191527968...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|   2015-01-01|2015-01-02T12:14:...|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|   2015-01-02|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|   2015-01-02|2015-01-02T12:15:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# incremental query\n",
    "# load data into a dataframe from glue data catalog starting and ending with a date/time\n",
    "# https://hudi.apache.org/docs/quick-start-guide/#incremental-query\n",
    "# https://hudi.apache.org/docs/configurations/#hoodiedatasourcereadendinstanttime\n",
    "\n",
    "begin_time = 20221208000000000\n",
    "end_time = 20221209000000000\n",
    "\n",
    "incremental_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': begin_time,\n",
    "    'hoodie.datasource.read.end.instanttime': end_time\n",
    "}\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**incremental_read_options) \\\n",
    "    .load(base_path)\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d601f81f-41b0-4f02-864f-cca9050c4bdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:51:56.601297Z",
     "iopub.status.busy": "2022-12-09T04:51:56.600848Z",
     "iopub.status.idle": "2022-12-09T04:51:58.880453Z",
     "shell.execute_reply": "2022-12-09T04:51:58.879657Z",
     "shell.execute_reply.started": "2022-12-09T04:51:56.601257Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bbd60de79e41adb67f0fcff8c96d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# add contents of dataframe to a temporary view and then query using SparkSQL\n",
    "\n",
    "incremental_df.createOrReplaceTempView('hudi_incremental')\n",
    "\n",
    "spark.sql(\n",
    "    'select * from `default`.`my_hudi_table` where id > 110 sort by id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd9682-e66c-4779-a7c3-57b9bd6b338d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
