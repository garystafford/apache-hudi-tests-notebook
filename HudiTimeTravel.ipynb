{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edd7973-3c88-4d92-8b2f-c69e8282c1ee",
   "metadata": {},
   "source": [
    "# Apache Hudi Query Tests\n",
    "\n",
    "Author: Gary Stafford\n",
    "Date: 2022-12-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c118d6a-ebe1-423a-a6a7-53b6c350d2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Amazon EMR cluster configuration used for this notebook\n",
    "\n",
    "# aws emr create-cluster \\\n",
    "#     --os-release-label 2.0.20221103.3 --termination-protected \\\n",
    "#     --applications Name=Hadoop Name=Spark Name=JupyterEnterpriseGateway Name=Livy Name=JupyterHub \\\n",
    "#     --ec2-attributes '{\"KeyName\":\"emr-demo-111222333444-us-east-1\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-067e7e7ee0fd99b2b\",\"EmrManagedSlaveSecurityGroup\":\"sg-0cc2191e5af867588\",\"EmrManagedMasterSecurityGroup\":\"sg-0be73448b3814bbc7\"}' \\\n",
    "#     --release-label emr-6.8.0 --log-uri 's3n://aws-logs-111222333444-us-east-1/elasticmapreduce/' \\\n",
    "#     --instance-groups '[{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":2}]},\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Master - 1\"},{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":2}]},\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Core - 2\"}]' \\\n",
    "#     --configurations '[{\"Classification\":\"spark-hive-site\",\"Properties\":{\"hive.metastore.client.factory.class\":\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\"}}]' \\\n",
    "#     --auto-scaling-role EMR_AutoScaling_DefaultRole --ebs-root-volume-size 100 --service-role EMR_DefaultRole --enable-debugging \\\n",
    "#     --name 'DemoCluster_6_8' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d00054-d9be-4ae5-8557-ab23ff52a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplied config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bca72-ce43-4eae-8334-0806a95eb4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%configure -f\n",
    "\n",
    "# { \"conf\": {\n",
    "#         \"spark.pyspark.virtualenv.enabled\": \"false\",\n",
    "#         \"spark.jars\": \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar\",\n",
    "#         \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "#         \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "#         \"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "#         \"spark.hadoop.hive.metastore.client.factory.class\": \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\",\n",
    "#         \"spark.hadoop.hive.metastore.schema.verification\": \"false\",\n",
    "#         \"spark.hadoop.fs.s3.canned.acl\": \"AuthenticatedRead\",\n",
    "#         \"spark.sql.catalogImplementation\": \"hive\"\n",
    "# }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c223a09-197b-4af9-ba64-dda37e6ccc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default aws hudi emr config\n",
    "# works fine\n",
    "# https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f3c9d-d8ad-4778-b3da-44585f2eadc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%configure -f\n",
    "\n",
    "# {\n",
    "#     \"conf\": {\n",
    "#         \"spark.jars\":\n",
    "#             \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "#         \"spark.serializer\":\n",
    "#             \"org.apache.spark.serializer.KryoSerializer\",\n",
    "#         \"spark.sql.hive.convertMetastoreParquet\":\n",
    "#             \"false\"\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221ea80-eb34-4605-91bb-6391aabedc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined configs - default aws hudi emr config and emr glue data catalog config\n",
    "# https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html\n",
    "# https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/metastore-config.html#glue-metastore\n",
    "\n",
    "# Note that \"spark.hadoop.hive.metastore.client.factory.class\" is extraneous since it was set in the EMR cluster config when provisioned (see first cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181640c1-a8ec-463b-bf50-3429ac705af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:38:23.936709Z",
     "iopub.status.busy": "2022-12-09T04:38:23.936487Z",
     "iopub.status.idle": "2022-12-09T04:38:58.546992Z",
     "shell.execute_reply": "2022-12-09T04:38:58.546293Z",
     "shell.execute_reply.started": "2022-12-09T04:38:23.936687Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>13</td><td>application_1670528992887_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0014/\" class=\"emr-proxy-link\" emr-resource=\"j-26M1ZRMTF8QYV\n",
       "\" application-id=\"application_1670528992887_0014\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-51.ec2.internal:8042/node/containerlogs/container_1670528992887_0014_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.hadoop.hive.metastore.client.factory.class': 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'}, 'proxyUser': 'assumed-role_AWSReservedSSO_AdministratorAccess_a3e1b00336d11e27_GaryStaffordAdminPersonal', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>10</td><td>application_1670528992887_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0011/\" class=\"emr-proxy-link\" emr-resource=\"j-26M1ZRMTF8QYV\n",
       "\" application-id=\"application_1670528992887_0011\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-51.ec2.internal:8042/node/containerlogs/container_1670528992887_0011_01_000001/livy\" >Link</a></td><td>None</td><td></td></tr><tr><td>13</td><td>application_1670528992887_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0014/\" class=\"emr-proxy-link\" emr-resource=\"j-26M1ZRMTF8QYV\n",
       "\" application-id=\"application_1670528992887_0014\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-51.ec2.internal:8042/node/containerlogs/container_1670528992887_0014_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\":\n",
    "            \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "        \"spark.serializer\":\n",
    "            \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.hive.convertMetastoreParquet\":\n",
    "            \"false\",\n",
    "        \"spark.hadoop.hive.metastore.client.factory.class\":\n",
    "            \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfaae400-5d9a-4b0d-bdff-3789a46d1990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:23:44.909117Z",
     "iopub.status.busy": "2022-12-09T01:23:44.908881Z",
     "iopub.status.idle": "2022-12-09T01:24:18.539187Z",
     "shell.execute_reply": "2022-12-09T01:24:18.538497Z",
     "shell.execute_reply.started": "2022-12-09T01:23:44.909094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670868a9efb549ed88c7628dc77060d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>12</td><td>application_1670528992887_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-139.ec2.internal:20888/proxy/application_1670528992887_0013/\" class=\"emr-proxy-link\" emr-resource=\"j-26M1ZRMTF8QYV\n",
       "\" application-id=\"application_1670528992887_0013\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-51.ec2.internal:8042/node/containerlogs/container_1670528992887_0013_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d335cbb-8b1b-4639-bbb5-62f0bc371015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:24:18.540854Z",
     "iopub.status.busy": "2022-12-09T01:24:18.540604Z",
     "iopub.status.idle": "2022-12-09T01:24:18.594946Z",
     "shell.execute_reply": "2022-12-09T01:24:18.594282Z",
     "shell.execute_reply.started": "2022-12-09T01:24:18.540821Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8cbb06011c4e29a393a4c484a24a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# amazon s3 path to hudi table key (sub-directory)\n",
    "\n",
    "base_path = 's3://open-data-lake-demo-us-east-1/hudi_demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371f070-3c87-4e6e-8d96-98a902e1703e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html#emr-hudi-dataframe\n",
    "\n",
    "# Create a DataFrame\n",
    "input_df = spark.createDataFrame([\n",
    "    (\"100\", \"2015-01-01\", \"2015-01-02T13:51:39.340396Z\"),\n",
    "    (\"101\", \"2015-01-01\", \"2015-01-02T12:14:58.597216Z\"),\n",
    "    (\"102\", \"2015-01-01\", \"2015-01-02T13:51:40.417052Z\"),\n",
    "    (\"103\", \"2015-01-01\", \"2015-01-02T13:51:40.519832Z\"),\n",
    "    (\"104\", \"2015-01-02\", \"2015-01-02T12:15:00.512679Z\"),\n",
    "    (\"105\", \"2015-01-02\", \"2015-01-02T13:51:42.248818Z\"),\n",
    "], [\"id\", \"creation_date\", \"last_update_time\"])\n",
    "\n",
    "# Specify common DataSourceWriteOptions in the single hudi options variable\n",
    "# https://hudi.apache.org/docs/configurations\n",
    "hudi_write_options = {\n",
    "    'hoodie.datasource.write.operation':\n",
    "        'upsert',\n",
    "    'hoodie.table.name':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.write.table.type':\n",
    "        'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field':\n",
    "        'id',\n",
    "    'hoodie.datasource.write.partitionpath.field':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.write.precombine.field':\n",
    "        'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable':\n",
    "        'true',\n",
    "    'hoodie.datasource.hive_sync.database':\n",
    "        'default',\n",
    "    'hoodie.datasource.hive_sync.table':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.hive_sync.partition_fields':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class':\n",
    "        'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.hive_sync.mode':\n",
    "        'hms'\n",
    "}\n",
    "\n",
    "# Write a DataFrame as a Hudi dataset\n",
    "input_df.write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**hudi_write_options) \\\n",
    "    .mode('append') \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "239590eb-cfd5-402c-84fd-a5e9bdaafeaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T03:39:46.253979Z",
     "iopub.status.busy": "2022-12-09T03:39:46.253752Z",
     "iopub.status.idle": "2022-12-09T03:40:37.673303Z",
     "shell.execute_reply": "2022-12-09T03:40:37.672682Z",
     "shell.execute_reply.started": "2022-12-09T03:39:46.253955Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a9d48beb3c4cd293b83200722f6481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "input_df = spark.createDataFrame([\n",
    "    (\"110\", \"2016-01-02\", \"2016-01-02T13:51:39.340396Z\"),\n",
    "    (\"111\", \"2016-01-02\", \"2016-01-02T12:14:58.597216Z\"),\n",
    "    (\"112\", \"2016-01-02\", \"2016-01-02T13:51:40.417052Z\"),\n",
    "    (\"113\", \"2016-01-02\", \"2016-01-02T13:51:40.519832Z\"),\n",
    "    (\"114\", \"2016-01-02\", \"2016-01-02T12:15:00.512679Z\"),\n",
    "    (\"115\", \"2016-01-02\", \"2016-01-02T13:51:42.248818Z\"),\n",
    "], [\"id\", \"creation_date\", \"last_update_time\"])\n",
    "\n",
    "# Specify common DataSourceWriteOptions in the single hudi options variable\n",
    "# https://hudi.apache.org/docs/configurations\n",
    "hudi_write_options = {\n",
    "    'hoodie.datasource.write.operation':\n",
    "        'upsert',\n",
    "    'hoodie.table.name':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.write.table.type':\n",
    "        'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field':\n",
    "        'id',\n",
    "    'hoodie.datasource.write.partitionpath.field':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.write.precombine.field':\n",
    "        'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable':\n",
    "        'true',\n",
    "    'hoodie.datasource.hive_sync.database':\n",
    "        'default',\n",
    "    'hoodie.datasource.hive_sync.table':\n",
    "        'my_hudi_table',\n",
    "    'hoodie.datasource.hive_sync.partition_fields':\n",
    "        'creation_date',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class':\n",
    "        'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.hive_sync.mode':\n",
    "        'hms'\n",
    "}\n",
    "\n",
    "# Write a DataFrame as a Hudi dataset\n",
    "input_df.write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**hudi_write_options) \\\n",
    "    .mode('append') \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2520f24-55d6-4994-9e9c-5b9de48f6e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T03:40:45.620403Z",
     "iopub.status.busy": "2022-12-09T03:40:45.620176Z",
     "iopub.status.idle": "2022-12-09T03:40:50.901573Z",
     "shell.execute_reply": "2022-12-09T03:40:50.900790Z",
     "shell.execute_reply.started": "2022-12-09T03:40:45.620381Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b75e1f325184c7ea65f1f85fec4741e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|creation_date|    last_update_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|  20221209033946565|20221209033946565...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|   2016-01-02|2016-01-02T12:15:...|\n",
      "|  20221209033946565|20221209033946565...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|   2016-01-02|2016-01-02T12:14:...|\n",
      "|  20221209033946565|20221209033946565...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209033946565|20221209033946565...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|   2015-01-01|2015-01-02T12:14:...|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|   2015-01-02|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|   2015-01-02|2015-01-02T12:15:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from s3\n",
    "\n",
    "snapshot_query_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load(base_path + '/*/*')\n",
    "\n",
    "snapshot_query_df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717a6e1b-8dcd-4328-809e-7430d1061a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:09.743095Z",
     "iopub.status.busy": "2022-12-09T01:25:09.742917Z",
     "iopub.status.idle": "2022-12-09T01:25:15.516820Z",
     "shell.execute_reply": "2022-12-09T01:25:15.492249Z",
     "shell.execute_reply.started": "2022-12-09T01:25:09.743074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bc71f00b254f1797b6315d602bd688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209005913239|20221209005913239...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|2015-01-02T12:14:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|2015-01-02T13:51:...|   2015-01-02|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|2015-01-02T12:15:...|   2015-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from glue data catalog\n",
    "\n",
    "glue_query_df = spark.sql(\"select * from `default`.`my_hudi_table`;\")\n",
    "\n",
    "glue_query_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a342c5-1a7d-4f93-86bf-b252f95191e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:15.519236Z",
     "iopub.status.busy": "2022-12-09T01:25:15.518985Z",
     "iopub.status.idle": "2022-12-09T01:25:16.798772Z",
     "shell.execute_reply": "2022-12-09T01:25:16.797974Z",
     "shell.execute_reply.started": "2022-12-09T01:25:15.519201Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7729fb5931c548ab99a91581412d4d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209005913239|20221209005913239...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|2015-01-02T12:14:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|2015-01-02T13:51:...|   2015-01-02|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|2015-01-02T12:15:...|   2015-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from glue data catalog\n",
    "\n",
    "as_of_time = '20221209033946565'\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('as.of.instant', as_of_time) \\\n",
    "    .table('default.my_hudi_table')\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78027b66-7e58-4729-ab22-d8921b659780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T03:43:01.143387Z",
     "iopub.status.busy": "2022-12-09T03:43:01.143165Z",
     "iopub.status.idle": "2022-12-09T03:43:01.906813Z",
     "shell.execute_reply": "2022-12-09T03:43:01.905981Z",
     "shell.execute_reply.started": "2022-12-09T03:43:01.143363Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f179fbb169354c95a6d27a7798811d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o88.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1034, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o88.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe from glue data catalog as of a date/time using SparkSQL\n",
    "# throws error: java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
    "\n",
    "as_of_time = '20221209005913239'\n",
    "\n",
    "incremental_df = spark.sql(\n",
    "    'select * from default.my_hudi_table timestamp as of ' + as_of_time + ';')\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ebaceb5-a6b9-4461-b554-f0d0566f5066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T04:26:41.045915Z",
     "iopub.status.busy": "2022-12-09T04:26:41.045686Z",
     "iopub.status.idle": "2022-12-09T04:26:41.778099Z",
     "shell.execute_reply": "2022-12-09T04:26:41.777445Z",
     "shell.execute_reply.started": "2022-12-09T04:26:41.045890Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o88.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1034, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1670528992887_0013/container_1670528992887_0013_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o88.sql.\n",
      ": java.lang.UnsupportedOperationException: Table default.my_hudi_table does not support time travel.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableNotSupportTimeTravelError(QueryCompilationErrors.scala:2412)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.failTimeTravel(V2SessionCatalog.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:70)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1072)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:215)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$6(RuleExecutor.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$RuleExecutionContext$.withContext(RuleExecutor.scala:327)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5(RuleExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$5$adapted(RuleExecutor.scala:274)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:274)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:213)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "select * from default.my_hudi_table timestamp as of 20221209005913239;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68985f5a-5984-42cd-a8b3-9b49b6a979ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:17.570948Z",
     "iopub.status.busy": "2022-12-09T01:25:17.570638Z",
     "iopub.status.idle": "2022-12-09T01:25:22.895302Z",
     "shell.execute_reply": "2022-12-09T01:25:22.894511Z",
     "shell.execute_reply.started": "2022-12-09T01:25:17.570913Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c249ee4a2cb4f5387e5c0f89087e0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|creation_date|    last_update_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|  20221209005913239|20221209005913239...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|   2016-01-02|2016-01-02T12:15:...|\n",
      "|  20221209005913239|20221209005913239...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209005913239|20221209005913239...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209005913239|20221209005913239...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|   2016-01-02|2016-01-02T12:14:...|\n",
      "|  20221209005913239|20221209005913239...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221209005913239|20221209005913239...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|   2015-01-01|2015-01-02T12:14:...|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|   2015-01-02|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|   2015-01-02|2015-01-02T12:15:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# incremental query\n",
    "# load data into a dataframe from glue data catalog starting as a date/time\n",
    "# https://hudi.apache.org/docs/quick-start-guide/#incremental-query\n",
    "\n",
    "instant_time = '20221208000000000'\n",
    "\n",
    "incremental_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': instant_time,\n",
    "}\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**incremental_read_options) \\\n",
    "    .load(base_path)\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89dd7d80-c405-457d-9846-aab25b09024b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:22.896755Z",
     "iopub.status.busy": "2022-12-09T01:25:22.896508Z",
     "iopub.status.idle": "2022-12-09T01:25:25.188824Z",
     "shell.execute_reply": "2022-12-09T01:25:25.174139Z",
     "shell.execute_reply.started": "2022-12-09T01:25:22.896721Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a967cc87dd41c39995f31819b9380f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209005913239|20221209005913239...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# add contents of dataframe to a temporary view and then query using SparkSQL\n",
    "\n",
    "incremental_df.createOrReplaceTempView('hudi_incremental')\n",
    "\n",
    "spark.sql(\n",
    "    'select * from `default`.`my_hudi_table` where id > 110 sort by id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786c6fc1-d033-4908-b425-fc3d45768292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:25.194188Z",
     "iopub.status.busy": "2022-12-09T01:25:25.193903Z",
     "iopub.status.idle": "2022-12-09T01:25:31.280680Z",
     "shell.execute_reply": "2022-12-09T01:25:30.931807Z",
     "shell.execute_reply.started": "2022-12-09T01:25:25.194151Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a87f62c0ca49cf8f338aada9f2bab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|_hoodie_commit_time|count|\n",
      "+-------------------+-----+\n",
      "|  20221209005913239|    6|\n",
      "|  20221208185224525|    6|\n",
      "+-------------------+-----+\n",
      "\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221208191527968|20221208191527968...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208191527968|20221208191527968...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|2015-01-02T12:14:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|2015-01-02T13:51:...|   2015-01-01|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|2015-01-02T13:51:...|   2015-01-02|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|2015-01-02T12:15:...|   2015-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# time travel query\n",
    "# https://hudi.apache.org/docs/0.11.1/quick-start-guide/#time-travel-query\n",
    "\n",
    "spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load(base_path) \\\n",
    "    .groupBy('_hoodie_commit_time').count().show()\n",
    "\n",
    "instant0 = '2022-12-08 00:00:00.000'  # before any records were created - should see no records\n",
    "\n",
    "instant1 = '2022-12-08 18:08:27.522'  # ids 100 - 105 created  - should see (6) new records\n",
    "\n",
    "instant2 = '2022-12-08 18:13:30.830'  # ids 110 - 115 created - should see (12) new records\n",
    "\n",
    "instant3 = '2022-12-08 18:52:24.525'  # ids 100 - 105 updated - should see (6) new records and (6) modified records\n",
    "\n",
    "instant4 = '2022-12-08 19:15:27.968'  # ids 110 - 115 updated - should see (12) modified records\n",
    "\n",
    "instant5 = '2022-12-09 00:59:13.239'  # ids 110 - 115 updated second time - should see (12) modified records\n",
    "\n",
    "instant6 = '2022-12-09 03:39:46.565'  # ids 110 - 115 updated third time - should see (12) modified records\n",
    "\n",
    "instant7 = '2022 12 09 00:00:00.000'  # after latest changes - should see (12) modified/latest records\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('as.of.instant', instant4) \\\n",
    "    .load(base_path)\n",
    "\n",
    "incremental_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8e75f6-740d-4561-8281-b7376f3ce4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:31.282916Z",
     "iopub.status.busy": "2022-12-09T01:25:31.282702Z",
     "iopub.status.idle": "2022-12-09T01:25:33.588832Z",
     "shell.execute_reply": "2022-12-09T01:25:33.587952Z",
     "shell.execute_reply.started": "2022-12-09T01:25:31.282891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76d1a746c9b4ba69c6243c8a55f63be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|creation_date|    last_update_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+\n",
      "|  20221208191527968|20221208191527968...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|   2016-01-02|2016-01-02T12:15:...|\n",
      "|  20221208191527968|20221208191527968...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208191527968|20221208191527968...|               110|            2016-01-02|1c996d1d-3fd5-4ac...|110|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208191527968|20221208191527968...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|   2016-01-02|2016-01-02T12:14:...|\n",
      "|  20221208191527968|20221208191527968...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208191527968|20221208191527968...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|   2016-01-02|2016-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               100|            2015-01-01|9a149870-8fd4-4a6...|100|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               101|            2015-01-01|9a149870-8fd4-4a6...|101|   2015-01-01|2015-01-02T12:14:...|\n",
      "|  20221208185224525|20221208185224525...|               102|            2015-01-01|9a149870-8fd4-4a6...|102|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               103|            2015-01-01|9a149870-8fd4-4a6...|103|   2015-01-01|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               105|            2015-01-02|e1c63f18-5cd3-472...|105|   2015-01-02|2015-01-02T13:51:...|\n",
      "|  20221208185224525|20221208185224525...|               104|            2015-01-02|e1c63f18-5cd3-472...|104|   2015-01-02|2015-01-02T12:15:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# incremental query\n",
    "# load data into a dataframe from glue data catalog starting and ending with a date/time\n",
    "# https://hudi.apache.org/docs/quick-start-guide/#incremental-query\n",
    "# https://hudi.apache.org/docs/configurations/#hoodiedatasourcereadendinstanttime\n",
    "\n",
    "begin_time = 20221208000000000\n",
    "end_time = 20221209000000000\n",
    "\n",
    "incremental_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': begin_time,\n",
    "    'hoodie.datasource.read.end.instanttime': end_time\n",
    "}\n",
    "\n",
    "incremental_df = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**incremental_read_options) \\\n",
    "    .load(base_path)\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d601f81f-41b0-4f02-864f-cca9050c4bdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:25:33.590613Z",
     "iopub.status.busy": "2022-12-09T01:25:33.590319Z",
     "iopub.status.idle": "2022-12-09T01:25:35.908009Z",
     "shell.execute_reply": "2022-12-09T01:25:35.907156Z",
     "shell.execute_reply.started": "2022-12-09T01:25:33.590578Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05102c60cb7949c294f13a46aecd119f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|    last_update_time|creation_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+\n",
      "|  20221209005913239|20221209005913239...|               111|            2016-01-02|1c996d1d-3fd5-4ac...|111|2016-01-02T12:14:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               112|            2016-01-02|1c996d1d-3fd5-4ac...|112|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               113|            2016-01-02|1c996d1d-3fd5-4ac...|113|2016-01-02T13:51:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               114|            2016-01-02|1c996d1d-3fd5-4ac...|114|2016-01-02T12:15:...|   2016-01-02|\n",
      "|  20221209005913239|20221209005913239...|               115|            2016-01-02|1c996d1d-3fd5-4ac...|115|2016-01-02T13:51:...|   2016-01-02|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+--------------------+-------------+"
     ]
    }
   ],
   "source": [
    "# add contents of dataframe to a temporary view and then query using SparkSQL\n",
    "\n",
    "incremental_df.createOrReplaceTempView('hudi_incremental')\n",
    "\n",
    "spark.sql(\n",
    "    'select * from `default`.`my_hudi_table` where id > 110 sort by id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd9682-e66c-4779-a7c3-57b9bd6b338d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
